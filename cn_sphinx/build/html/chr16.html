<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>第16章 监督学习方法 &mdash; biopython_cn 0.1 documentation</title>
    
    <link rel="stylesheet" href="_static/default.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="biopython_cn 0.1 documentation" href="index.html" />
    <link rel="next" title="第17章 Graphics模块中的基因组可视化包—GenomeDiagram" href="chr17.html" />
    <link rel="prev" title="第15章 聚类分析" href="chr15.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="chr17.html" title="第17章 Graphics模块中的基因组可视化包—GenomeDiagram"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="chr15.html" title="第15章 聚类分析"
             accesskey="P">previous</a> |</li>
        <li><a href="index.html">biopython_cn 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="id1">
<h1>第16章 监督学习方法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>注意本章介绍的所有监督学习方法都需要先安装Numerical Python （numpy）。</p>
<div class="section" id="logistic">
<h2>16.1 Logistic 回归模型<a class="headerlink" href="#logistic" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id2">
<h3>16.1.1 背景和目的<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>Logistic回归是一种监督学习方法，通过若干预测变量 <em>x</em><sub>*i*</sub> 的加权和来尝试将样本划分为 <em>K</em> 个不同类别。Logistic回归模型可用来计算预测变量的权重 β<sub>*i*</sub> 。在Biopython中，logistic回归模型目前只实现了二类别（ <em>K</em> = 2 ）分类，而预测变量的数量没有限制。</p>
<p>作为一个例子，我们试着预测细菌中的操纵子结构。一个操纵子是在一条DNA链上许多相邻基因组成的一个集合，可以被共同转录为一条mRNA分子。这条mRNA分子经翻译后产生多个不同的蛋白质。我们将以枯草芽孢杆菌的操纵子数据进行说明，它的一个操纵子平均包含2.4个基因。</p>
<p>作为理解细菌的基因调节的第一步，我们需要知道其操纵子的结构。枯草芽孢杆菌大约10%的基因操纵子结构已经通过实验获知。剩下的90%的基因操纵子结构可以通过一种监督学习方法来预测。</p>
<p>在这种监督学习方法中，我们需要选择某些与操纵子结构有关的容易度量的预测变量 <em>x</em><sub>*i*</sub> 。例如可以选择基因间碱基对距离来来作为其中一个预测变量。同一个操纵子中的相邻基因往往距离相对较近，而位于不同操纵子的相邻基因间通常具有更大的空间来容纳启动子和终止子序列。另一个预测变量可以基于基因表达量度。根据操纵子的定义，属于同一个操纵子的基因有相同的基因表达谱，而不同操纵子的两个基因的表达谱也不相同。在实际操作中，由于存在测量误差，对相同操纵子的基因表达轮廓的测量不会完全一致。为了测量基因表达轮廓的相似性，我们假设测量误差服从正态分布，然后计算对应的对数似然分值。</p>
<p>现在我们有了两个预测变量，可以据此预测在同一条DNA链上两个相邻基因是否属于相同的操纵子：
-  <em>x</em><sub>1</sub> ：两基因间的碱基对数；
-  <em>x</em><sub>2</sub> ：两基因表达谱的相似度。</p>
<p>在logistic回归模型中，我们使用这两个预测变量的加权和来计算一个联合得分 <em>S</em>：</p>
<div class="math">
\[\begin{equation}
S = \beta_0 + \beta_1 x_1 + \beta_2 x_2.
\end{equation}\]</div>
<p>根据下面两组示例基因，logistic回归模型对参数 β<sub>0</sub> ， β<sub>1</sub>, β<sub>2</sub> 给出合适的值：
-  OP: 相邻基因，相同DNA链，属于相同操纵子；
-  NOP: 相邻基因，相同DNA链，属于不同操纵子。</p>
<p>在logistic回归模型中，属于某个类别的概率依赖于通过logistic函数得出的分数。对于这两类OP和NOP，相应概率可如下表述：</p>
<div class="math">
\[\begin{split}\begin{eqnarray}
\Pr(\mathrm{OP}|x_1, x_2) &amp; = &amp; \frac{\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)}{1+\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} \label{eq:OP} \\
\Pr(\mathrm{NOP}|x_1, x_2) &amp; = &amp; \frac{1}{1+\exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)} \label{eq:NOP}
\end{eqnarray}\end{split}\]</div>
<p>使用一组已知是否属于相同操纵子（OP类别）或不同操纵子（NOP类别）的基因对，通过最大化相应概率函数的对数似然值，我们可以计算权重 β<sub>0</sub>, β<sub>1</sub>, β<sub>2</sub> 。
(<a class="reference external" href="#eq:OP">16.2</a>) 和 (<a class="reference external" href="#eq:NOP">16.3</a>).</p>
</div>
<div class="section" id="id3">
<h3>16.1.2 训练logistic回归模型<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<hr class="docutils" />
<table border="1" class="docutils">
<colgroup>
<col width="100%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>表16.1： 已知类别(OP or NOP)的相邻基因对.如果两个基因相重叠，其基因间距离为负值</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils">
<colgroup>
<col width="19%" />
<col width="35%" />
<col width="38%" />
<col width="8%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>基因对</td>
<td>基因间距离 (<em>x</em><sub>1</sub>)</td>
<td>基因表达得分 (<em>x</em><sub>2</sub>)</td>
<td>类别</td>
</tr>
<tr class="row-even"><td><em>cotJA</em> — <em>cotJB</em></td>
<td>-53</td>
<td>-200.78</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>yesK</em> — <em>yesL</em></td>
<td>117</td>
<td>-267.14</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>lplA</em> — <em>lplB</em></td>
<td>57</td>
<td>-163.47</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>lplB</em> — <em>lplC</em></td>
<td>16</td>
<td>-190.30</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>lplC</em> — <em>lplD</em></td>
<td>11</td>
<td>-220.94</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>lplD</em> — <em>yetF</em></td>
<td>85</td>
<td>-193.94</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>yfmT</em> — <em>yfmS</em></td>
<td>16</td>
<td>-182.71</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>yfmF</em> — <em>yfmE</em></td>
<td>15</td>
<td>-180.41</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>citS</em> — <em>citT</em></td>
<td>-26</td>
<td>-181.73</td>
<td>OP</td>
</tr>
<tr class="row-odd"><td><em>citM</em> — <em>yflN</em></td>
<td>58</td>
<td>-259.87</td>
<td>OP</td>
</tr>
<tr class="row-even"><td><em>yfiI</em> — <em>yfiJ</em></td>
<td>126</td>
<td>-414.53</td>
<td>NOP</td>
</tr>
<tr class="row-odd"><td><em>lipB</em> — <em>yfiQ</em></td>
<td>191</td>
<td>-249.57</td>
<td>NOP</td>
</tr>
<tr class="row-even"><td><em>yfiU</em> — <em>yfiV</em></td>
<td>113</td>
<td>-265.28</td>
<td>NOP</td>
</tr>
<tr class="row-odd"><td><em>yfhH</em> — <em>yfhI</em></td>
<td>145</td>
<td>-312.99</td>
<td>NOP</td>
</tr>
<tr class="row-even"><td><em>cotY</em> — <em>cotX</em></td>
<td>154</td>
<td>-213.83</td>
<td>NOP</td>
</tr>
<tr class="row-odd"><td><em>yjoB</em> — <em>rapA</em></td>
<td>147</td>
<td>-380.85</td>
<td>NOP</td>
</tr>
<tr class="row-even"><td><em>ptsI</em> — <em>splA</em></td>
<td>93</td>
<td>-291.13</td>
<td>NOP</td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>表`16.1 &lt;#table:training&gt;`__ 列出了枯草芽孢杆菌的一些基因对，这些基因的操纵子结构已知。让我们根据表中的这些数据来计算其logistic回归模型：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">Bio</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">xs</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">53</span><span class="p">,</span> <span class="o">-</span><span class="mf">200.78</span><span class="p">],</span>
<span class="go">          [117, -267.14],</span>
<span class="go">          [57, -163.47],</span>
<span class="go">          [16, -190.30],</span>
<span class="go">          [11, -220.94],</span>
<span class="go">          [85, -193.94],</span>
<span class="go">          [16, -182.71],</span>
<span class="go">          [15, -180.41],</span>
<span class="go">          [-26, -181.73],</span>
<span class="go">          [58, -259.87],</span>
<span class="go">          [126, -414.53],</span>
<span class="go">          [191, -249.57],</span>
<span class="go">          [113, -265.28],</span>
<span class="go">          [145, -312.99],</span>
<span class="go">          [154, -213.83],</span>
<span class="go">          [147, -380.85],</span>
<span class="go">          [93, -291.13]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          1,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0,</span>
<span class="go">          0]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
<p>这里， <tt class="docutils literal"><span class="pre">xs</span></tt> 和 <tt class="docutils literal"><span class="pre">ys</span></tt> 是训练数据： <tt class="docutils literal"><span class="pre">xs</span></tt> 包含每个基因对的预测变量， <tt class="docutils literal"><span class="pre">ys</span></tt> 指定是否这个基因对属于相同操纵子（ <tt class="docutils literal"><span class="pre">1</span></tt> ，类别OP）或不同操纵子（<tt class="docutils literal"><span class="pre">0</span></tt>，类别NOP）。Logistic回归模型结果存储在 <tt class="docutils literal"><span class="pre">model</span></tt> 中，包含权重 β<sub>0</sub>, β<sub>1</sub>, and β<sub>2</sub>:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">beta</span>
<span class="go">[8.9830290157144681, -0.035968960444850887, 0.02181395662983519]</span>
</pre></div>
</div>
<p>注意 β<sub>1</sub> 是负的，这是因为具有更短基因间距离的基因对有更高的概率属于相同操纵子（类别OP）。另一方面， β<sub>2</sub> 为正，因为属于相同操纵子的基因对通常有更高的基因表达谱相似性得分。参数 β<sub>0</sub> 是正值是因为在这个训练数据中操纵子基因对占据大多数。</p>
<p>函数 <tt class="docutils literal"><span class="pre">train</span></tt> 有两个可选参数： <tt class="docutils literal"><span class="pre">update_fn</span></tt> 和 <tt class="docutils literal"><span class="pre">typecode</span></tt> 。 <tt class="docutils literal"><span class="pre">update_fn</span></tt> 可用来指定一个回调函数，以迭代数和对数似然值做参数。在这个例子中，我们可以使用这个回调函数追踪模型计算（使用Newton-Raphson迭代来最大化logistic回归模型的对数似然函数）进度：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">show_progress</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
<span class="go">        print &quot;Iteration:&quot;, iteration, &quot;Log-likelihood function:&quot;, loglikelihood</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">update_fn</span><span class="o">=</span><span class="n">show_progress</span><span class="p">)</span>
<span class="go">Iteration: 0 Log-likelihood function: -11.7835020695</span>
<span class="go">Iteration: 1 Log-likelihood function: -7.15886767672</span>
<span class="go">Iteration: 2 Log-likelihood function: -5.76877209868</span>
<span class="go">Iteration: 3 Log-likelihood function: -5.11362294338</span>
<span class="go">Iteration: 4 Log-likelihood function: -4.74870642433</span>
<span class="go">Iteration: 5 Log-likelihood function: -4.50026077146</span>
<span class="go">Iteration: 6 Log-likelihood function: -4.31127773737</span>
<span class="go">Iteration: 7 Log-likelihood function: -4.16015043396</span>
<span class="go">Iteration: 8 Log-likelihood function: -4.03561719785</span>
<span class="go">Iteration: 9 Log-likelihood function: -3.93073282192</span>
<span class="go">Iteration: 10 Log-likelihood function: -3.84087660929</span>
<span class="go">Iteration: 11 Log-likelihood function: -3.76282560605</span>
<span class="go">Iteration: 12 Log-likelihood function: -3.69425027154</span>
<span class="go">Iteration: 13 Log-likelihood function: -3.6334178602</span>
<span class="go">Iteration: 14 Log-likelihood function: -3.57900855837</span>
<span class="go">Iteration: 15 Log-likelihood function: -3.52999671386</span>
<span class="go">Iteration: 16 Log-likelihood function: -3.48557145163</span>
<span class="go">Iteration: 17 Log-likelihood function: -3.44508206139</span>
<span class="go">Iteration: 18 Log-likelihood function: -3.40799948447</span>
<span class="go">Iteration: 19 Log-likelihood function: -3.3738885624</span>
<span class="go">Iteration: 20 Log-likelihood function: -3.3423876581</span>
<span class="go">Iteration: 21 Log-likelihood function: -3.31319343769</span>
<span class="go">Iteration: 22 Log-likelihood function: -3.2860493346</span>
<span class="go">Iteration: 23 Log-likelihood function: -3.2607366863</span>
<span class="go">Iteration: 24 Log-likelihood function: -3.23706784091</span>
<span class="go">Iteration: 25 Log-likelihood function: -3.21488073614</span>
<span class="go">Iteration: 26 Log-likelihood function: -3.19403459259</span>
<span class="go">Iteration: 27 Log-likelihood function: -3.17440646052</span>
<span class="go">Iteration: 28 Log-likelihood function: -3.15588842703</span>
<span class="go">Iteration: 29 Log-likelihood function: -3.13838533947</span>
<span class="go">Iteration: 30 Log-likelihood function: -3.12181293595</span>
<span class="go">Iteration: 31 Log-likelihood function: -3.10609629966</span>
<span class="go">Iteration: 32 Log-likelihood function: -3.09116857282</span>
<span class="go">Iteration: 33 Log-likelihood function: -3.07696988017</span>
<span class="go">Iteration: 34 Log-likelihood function: -3.06344642288</span>
<span class="go">Iteration: 35 Log-likelihood function: -3.05054971191</span>
<span class="go">Iteration: 36 Log-likelihood function: -3.03823591619</span>
<span class="go">Iteration: 37 Log-likelihood function: -3.02646530573</span>
<span class="go">Iteration: 38 Log-likelihood function: -3.01520177394</span>
<span class="go">Iteration: 39 Log-likelihood function: -3.00441242601</span>
<span class="go">Iteration: 40 Log-likelihood function: -2.99406722296</span>
<span class="go">Iteration: 41 Log-likelihood function: -2.98413867259</span>
</pre></div>
</div>
<p>一旦对数似然函数得分增加值小于0.01，迭代将终止。如果在500次迭代后还没有到达收敛， <tt class="docutils literal"><span class="pre">train</span></tt> 函数返回并抛出一个 <tt class="docutils literal"><span class="pre">AssertionError</span></tt> 。</p>
<p>可选的关键字 <tt class="docutils literal"><span class="pre">typecode</span></tt> 几乎可以一直忽略。这个关键字允许用户选择要使用的数值矩阵类型。当为了避免大数据计算的内存问题时，可能有必要使用单精度浮点数（Float8，Float16等等）而不是默认的double型。</p>
</div>
<div class="section" id="id4">
<h3>16.1.3 使用logistic回归模型进行分类<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>调用 <tt class="docutils literal"><span class="pre">classify</span></tt> 函数可以进行分类。给定一个logistic回归模型和 <em>x</em><sub>1</sub> 和 <em>x</em><sub>2</sub> 的值（例如，操纵子结构未知的基因对）， <tt class="docutils literal"><span class="pre">classify</span></tt> 函数返回 <tt class="docutils literal"><span class="pre">1</span></tt> 或 <tt class="docutils literal"><span class="pre">0</span></tt> ，分别对应类别OP和NOP。例如，考虑基因对 <em>yxcE</em> ， <em>yxcD</em> 和 <em>yxiB</em> ， <em>yxiA</em> ：</p>
<hr class="docutils" />
<table border="1" class="docutils">
<colgroup>
<col width="100%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>表16.2：操纵子状态未知的相邻基因对</td>
</tr>
</tbody>
</table>
<table border="1" class="docutils">
<colgroup>
<col width="20%" />
<col width="38%" />
<col width="41%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td>基因对</td>
<td>基因间距离 <em>x</em><sub>1</sub></td>
<td>基因表达得分 <em>x</em><sub>2</sub></td>
</tr>
<tr class="row-even"><td><em>yxcE</em> — <em>yxcD</em></td>
<td>6</td>
<td>-173.143442352</td>
</tr>
<tr class="row-odd"><td><em>yxiB</em> — <em>yxiA</em></td>
<td>309</td>
<td>-271.005880394</td>
</tr>
</tbody>
</table>
<hr class="docutils" />
<p>Logistic回归模型预测 <em>yxcE</em> ， <em>yxcD</em> 属于相同操纵子（类别OP），而 <em>yxiB</em> ， <em>yxiA</em> 属于不同操纵子:</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">173.143442352</span><span class="p">])</span>
<span class="go">yxcE, yxcD: 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxiB, yxiA:&quot;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">])</span>
<span class="go">yxiB, yxiA: 0</span>
</pre></div>
</div>
<p>（这个结果和生物学文献报道的一致）。</p>
<p>为了确定这个预测的可信度，我们可以调用 <tt class="docutils literal"><span class="pre">calculate</span></tt> 函数来获得类别OP和NOP的概率(公式
(<a class="reference external" href="#eq:OP">16.2</a>) 和 (<a class="reference external" href="#eq:NOP">16.3</a>))。对于 <em>yxcE</em>, <em>yxcD</em> 我们发现</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">173.143442352</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: probability =&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s">&quot;class NOP: probability =&quot;</span><span class="p">,</span> <span class="n">q</span>
<span class="go">class OP: probability = 0.993242163503 class NOP: probability = 0.00675783649744</span>
</pre></div>
</div>
<p>对于 <em>yxiB</em> ， <em>yxiA</em></p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: probability =&quot;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s">&quot;class NOP: probability =&quot;</span><span class="p">,</span> <span class="n">q</span>
<span class="go">class OP: probability = 0.000321211251817 class NOP: probability = 0.999678788748</span>
</pre></div>
</div>
<p>为了确定回归模型的预测精确性，我们可以把模型应用到训练数据上：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, LogisticRegression.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
</pre></div>
</div>
<p>这表示除一个基因对外其他所有预测都是正确的。Leave-one-out分析可以对预测精确性给出一个更可信的估计。Leave-one-out是指从训练数据中移除要预测的基因重新计算模型，再用该模型进行预测比对：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        model = LogisticRegression.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:])</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, LogisticRegression.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
</pre></div>
</div>
<p>Leave-one-out分析显示这个logistic回归模型的预测只对两个基因对不正确，对应预测精确度为88%。</p>
</div>
<div class="section" id="id5">
<h3>16.1.4 Logistic回归，线性判别分析和支持向量机<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>Logistic回归模型类似于线性判别分析。在线性判别分析中，类别概率同样可由方程(<a class="reference external" href="#eq:OP">16.2</a>) and (<a class="reference external" href="#eq:NOP">16.3</a>)给出。但是，不是直接估计系数β，我们首先对预测变量 <em>x</em> 拟合一个正态分布。然后通过这个正态分布的平均值和方差计算系数β。如果 <em>x</em> 的分布确实是正态的，线性判别分析将比logistic回归模型有更好的性能。另一方面，logistic回归模型对于偏态到正态的广泛分布更加强健。</p>
<p>另一个相似的方法是应用线性核函数的支持向量机。这样的SVM也使用一个预测变量的线性组合，但是是从靠近类别之间的边界区域的预测变量 <em>x</em> 来估计系数β。如果logistic回归模型(公式 (<a class="reference external" href="#eq:OP">16.2</a>) 和 (<a class="reference external" href="#eq:NOP">16.3</a>))很好的描述了远离边界区域的 <em>x</em> ，我们可以期望logistic回归模型优于线性核函数SVM，因为它应用了更多数据。如果不是，SVM可能更好。</p>
<p>Trevor Hastie, Robert Tibshirani, and Jerome Friedman: The Elements of Statistical Learning. Data Mining, Inference, and Prediction.(统计学习基础:数据挖掘、推理与预测) Springer Series in Statistics, 2001. 4.4章.</p>
</div>
</div>
<div class="section" id="k-knn">
<h2>16.2 <em>k</em>-最近邻居法（ <em>KNN</em> ）<a class="headerlink" href="#k-knn" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id6">
<h3>16.2.1 背景和目的<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>最近邻居法是一种不需要将数据拟合到一个模型的监督学习算法。数据点是基于训练数据集的 <em>k</em> 个最近邻居类别进行分类的。</p>
<p>在Biopython中， <em>KNN</em> 方法可在 <tt class="docutils literal"><span class="pre">Bio.KNN</span></tt> 中获得。我们使用 <cite>16.1 &lt;#sec:LogisticRegression&gt;</cite> 同样的操纵子数据集来说明Biopython中 <em>KNN</em> 方法的用法。</p>
</div>
<div class="section" id="knn">
<h3>16.2.2 初始化一个 <em>KNN</em> 模型<a class="headerlink" href="#knn" title="Permalink to this headline">¶</a></h3>
<p>使用表`16.1 &lt;#table:training&gt;`__中的数据，我们创建和初始化一个*KNN*模型：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">Bio</span> <span class="kn">import</span> <span class="n">kNN</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</pre></div>
</div>
<p>这里 <tt class="docutils literal"><span class="pre">xs</span></tt> 和 <tt class="docutils literal"><span class="pre">ys</span></tt> 和 <a class="reference external" href="#subsec:LogisticRegressionTraining">16.1.2</a> 中的相同。 <tt class="docutils literal"><span class="pre">k</span></tt> 是分类中的邻居数 <em>k</em> 。对于二分类，为 <em>k</em> 选择一个奇数可以避免tied votes。函数名 <tt class="docutils literal"><span class="pre">train</span></tt> 在这里有点不合适，因为就没有训练模型：这个函数仅仅是用来存储模型变量 <tt class="docutils literal"><span class="pre">xs</span></tt> ， <tt class="docutils literal"><span class="pre">ys</span></tt> 和 <tt class="docutils literal"><span class="pre">k</span></tt> 。</p>
</div>
<div class="section" id="id7">
<h3>16.2.3 使用*KNN* 模型来分类<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>应用 <em>KNN</em> 模型对新数据进行分类，我们使用 <tt class="docutils literal"><span class="pre">classify</span></tt> 函数。这个函数以一个数据点(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>)为参数并在训练数据集 <tt class="docutils literal"><span class="pre">xs</span></tt> 中寻找 <em>k</em> -最近邻居。然后基于在这 <em>k</em> 个邻居中出现最多的类别（ <tt class="docutils literal"><span class="pre">ys</span></tt> ）来对数据点(<em>x</em><sub>1</sub>,<em>x</em><sub>2</sub>)进行分类。</p>
<p>对于基因对 <em>yxcE</em> 、 <em>yxcD</em> 和 <em>yxiB</em> 、 <em>yxiA</em> 的例子，我们发现：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">yxcE, yxcD: 1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxiB, yxiA:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="go">yxiB, yxiA: 0</span>
</pre></div>
</div>
<p>和logistic回归模型一致，<em>yxcE</em>,*yxcD*被归为一类（类别OP），<em>yxiB</em>,*yxiA*属于不同操纵子。</p>
<p>函数 <tt class="docutils literal"><span class="pre">classify</span></tt> 可以指定距离函数和权重函数作为可选参数。距离函数影响作为最近邻居的 <em>k</em> 个类别的选择，因为这些到查询点（ <em>x</em> ， <em>y</em> ）有最小距离的类别被定义为邻居。默认使用欧几里德距离。另外，我们也可以如示例中的使用曼哈顿距离：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">cityblock</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>   <span class="k">return</span> <span class="n">distance</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">distance_fn</span> <span class="o">=</span> <span class="n">cityblock</span><span class="p">)</span>
<span class="go">yxcE, yxcD: 1</span>
</pre></div>
</div>
<p>权重函数可以用于权重投票。例如，相比于相邻较远的邻居，我们可能想给更近的邻居一个更高的权重：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
<span class="gp">... </span>   <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;yxcE, yxcD:&quot;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight_fn</span> <span class="o">=</span> <span class="n">weight</span><span class="p">)</span>
<span class="go">yxcE, yxcD: 1</span>
</pre></div>
</div>
<p>默认所有邻居有相同权重。</p>
<p>为了确定这些预测的置信度，我们可以调用函数 <tt class="docutils literal"><span class="pre">calculate</span></tt> 来计算分配到类别OP和NOP的总权重。对于默认的加权方案，这样减少了每个分类的邻居数量。对于 <em>yxcE</em> ， <em>yxcD</em> ， 我们发现</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">&quot;class NOP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">class OP: weight = 0.0 class NOP: weight = 3.0</span>
</pre></div>
</div>
<p>这意味着 <tt class="docutils literal"><span class="pre">x1</span></tt> ， <tt class="docutils literal"><span class="pre">x2</span></tt> 的所有三个邻居都属于NOP类别。对另一个例子 <em>yesK</em> ， <em>yesL</em> 我们发现</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">117</span><span class="p">,</span> <span class="o">-</span><span class="mf">267.14</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">weight</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">print</span> <span class="s">&quot;class OP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s">&quot;class NOP: weight =&quot;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">class OP: weight = 2.0 class NOP: weight = 1.0</span>
</pre></div>
</div>
<p>这意思是两个邻居是操纵子对，另一个是非操纵子对</p>
<p>对于*KNN*方法的预测精确性，我们对训练数据应用模型：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, kNN.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
</pre></div>
</div>
<p>显示除了两个基因对所有预测都是正确的。Leave-one-out分析可以对预测精确性给出一个更可信的估计，这是通过从训练数据中移除要预测的基因，再重新计算模型实现：</p>
<div class="code python highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
<span class="go">        model = kNN.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:])</span>
<span class="go">        print &quot;True:&quot;, ys[i], &quot;Predicted:&quot;, kNN.classify(model, xs[i])</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 1</span>
<span class="go">True: 1 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 1</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 0</span>
<span class="go">True: 0 Predicted: 1</span>
</pre></div>
</div>
<p>Leave-one-out分析显示这个 <em>KNN</em> 模型的预测正确17个基因对中的13个，对应预测精确度为76%。</p>
</div>
</div>
<div class="section" id="naive">
<h2>16.3 Naive贝叶斯<a class="headerlink" href="#naive" title="Permalink to this headline">¶</a></h2>
<p>这部分将描述模块 <tt class="docutils literal"><span class="pre">Bio.NaiveBayes</span></tt> .</p>
</div>
<div class="section" id="id8">
<h2>16.4 最大熵<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>这部分将描述模块 <tt class="docutils literal"><span class="pre">Bio.MaximumEntropy</span></tt>.</p>
</div>
<div class="section" id="id9">
<h2>16.5  马尔科夫模型<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>这部分将描述模块 <tt class="docutils literal"><span class="pre">Bio.MarkovModel</span></tt> 和/或
<tt class="docutils literal"><span class="pre">Bio.HMM.MarkovModel</span></tt> .</p>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">第16章 监督学习方法</a><ul>
<li><a class="reference internal" href="#logistic">16.1 Logistic 回归模型</a><ul>
<li><a class="reference internal" href="#id2">16.1.1 背景和目的</a></li>
<li><a class="reference internal" href="#id3">16.1.2 训练logistic回归模型</a></li>
<li><a class="reference internal" href="#id4">16.1.3 使用logistic回归模型进行分类</a></li>
<li><a class="reference internal" href="#id5">16.1.4 Logistic回归，线性判别分析和支持向量机</a></li>
</ul>
</li>
<li><a class="reference internal" href="#k-knn">16.2 <em>k</em>-最近邻居法（ <em>KNN</em> ）</a><ul>
<li><a class="reference internal" href="#id6">16.2.1 背景和目的</a></li>
<li><a class="reference internal" href="#knn">16.2.2 初始化一个 <em>KNN</em> 模型</a></li>
<li><a class="reference internal" href="#id7">16.2.3 使用*KNN* 模型来分类</a></li>
</ul>
</li>
<li><a class="reference internal" href="#naive">16.3 Naive贝叶斯</a></li>
<li><a class="reference internal" href="#id8">16.4 最大熵</a></li>
<li><a class="reference internal" href="#id9">16.5  马尔科夫模型</a></li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="chr15.html"
                        title="previous chapter">第15章 聚类分析</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="chr17.html"
                        title="next chapter">第17章 Graphics模块中的基因组可视化包—GenomeDiagram</a></p>
  <h3>This Page</h3>
  <ul class="this-page-menu">
    <li><a href="_sources/chr16.txt"
           rel="nofollow">Show Source</a></li>
  </ul>
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="chr17.html" title="第17章 Graphics模块中的基因组可视化包—GenomeDiagram"
             >next</a> |</li>
        <li class="right" >
          <a href="chr15.html" title="第15章 聚类分析"
             >previous</a> |</li>
        <li><a href="index.html">biopython_cn 0.1 documentation</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2013, biopythoners.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.
    </div>
  </body>
</html>